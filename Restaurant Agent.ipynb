{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2bcd1a",
   "metadata": {},
   "source": [
    "# ðŸ½ï¸ Restaurant Recommendation Agent using AWS Strands with **Observability and Evaluation**\n",
    "\n",
    "#### What we will build:\n",
    "\n",
    "- **Local Vector DB**: A searchable collection of restaurant information that the agent can query.\n",
    "- **Strands Agent**: An AI assistant that can recommend restaurants based on their preferences.\n",
    "- **LangFuse**: A tool that lets us \"see\" how our agent works and makes decisions.\n",
    "- **RAGAS**: A framework that helps us evaluate how well our agent is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bc9fa",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Required packages\n",
    "\n",
    "- **langchain** - helps with splitting documents into chunks\n",
    "- **langfuse** - observability for the agent\n",
    "- **ragas** - evaluate agent's performanse\n",
    "- **chromadb** - storing and searching vector embeddings\n",
    "- **boto3** - AWS SDK for Python, access AWS services and use AWS Bedrock models\n",
    "- **sentence-transformers** - using Huggingface library for text embeddings\n",
    "- **strands-agents** - building AI agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62868cd8",
   "metadata": {},
   "source": [
    "### ðŸ’¿ Create vectorstore from Restaurant data\n",
    "\n",
    "A Vector database stores text as vectors that represent the meaning of the text. This allows us to search for similar meanings, not just extract word matches. \n",
    "\n",
    "**For example:** If we search for *\"vegetarian food\"*, we might also find results about *\"plant-based dishes\"*, even if those exact words aren't used. \n",
    "\n",
    "Create a vector database using data files in the `data` folder. Those are **Markdown** files containing information about different restaurants, their menus and their specialties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a211be",
   "metadata": {},
   "source": [
    "#### Load enviroment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b25743b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY environment variable set: sk-pr...fzbAA\n",
      "âœ… HF_TOKEN environment variable set: hf_zu...hMlKE\n",
      "âœ… GROQ_API_KEY environment variable set: gsk_1...n45ix\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"âš ï¸ OPENAI_API_KEY environment variable not set\")\n",
    "else:\n",
    "    print(f\"âœ… OPENAI_API_KEY environment variable set: {OPENAI_API_KEY[:5]}...{OPENAI_API_KEY[-5:]}\")\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"âš ï¸ HF_TOKEN environment variable not set\")\n",
    "else:\n",
    "    print(f\"âœ… HF_TOKEN environment variable set: {HF_TOKEN[:5]}...{HF_TOKEN[-5:]}\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"âš ï¸ GROQ_API_KEY environment variable not set\")\n",
    "else:\n",
    "    print(f\"âœ… GROQ_API_KEY environment variable set: {GROQ_API_KEY[:5]}...{GROQ_API_KEY[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d12f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for document loading and processing\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b761075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 markdown files in ./data\n",
      "============================================================\n",
      "Loading: restaurant_bella_luna.md\n",
      "  - Loaded 1 document(s)\n",
      "Loading: restaurant_verde_garden.md\n",
      "  - Loaded 1 document(s)\n",
      "Loading: restaurant_dragon_palace.md\n",
      "  - Loaded 1 document(s)\n",
      "============================================================\n",
      "ðŸ“š Total documents loaded: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './data'\n",
    "assert os.path.exists(DATA_PATH), f\"Data path {DATA_PATH} does not exist\"\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"\n",
    "    Load all markdown documents from a folder.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing markdown files\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        raise ValueError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    # Find all markdown files in the folder\n",
    "    md_files = list(folder.glob(\"*.md\"))\n",
    "    \n",
    "    if not md_files:\n",
    "        print(f\"Warning: No markdown files found in {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(md_files)} markdown files in {folder_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    # Loop through each markdown file\n",
    "    for md_file in md_files:\n",
    "        print(f\"Loading: {md_file.name}\")\n",
    "        \n",
    "        # Load the markdown document\n",
    "        loader = UnstructuredMarkdownLoader(str(md_file))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add source metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata['source_file'] = md_file.name\n",
    "            doc.metadata['source_path'] = str(md_file)\n",
    "        \n",
    "        all_documents.extend(documents)\n",
    "        print(f\"  - Loaded {len(documents)} document(s)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ðŸ“š Total documents loaded: {len(all_documents)}\\n\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "all_documents: list[Document] = load_documents(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f63fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 3 documents into chunks...\n",
      "  - Chunk size: 100 characters\n",
      "  - Chunk overlap: 10 characters\n",
      "============================================================\n",
      "Created 325 total chunks\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents, chunk_size=100, chunk_overlap=10):\n",
    "    \"\"\"\n",
    "    Split documents into chunks.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of Document objects to split\n",
    "        chunk_size: Maximum size of each chunk in characters\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of document chunks\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        print(\"Warning: No documents to split\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Splitting {len(documents)} documents into chunks...\")\n",
    "    print(f\"  - Chunk size: {chunk_size} characters\")\n",
    "    print(f\"  - Chunk overlap: {chunk_overlap} characters\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split all documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} total chunks\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f267392",
   "metadata": {},
   "source": [
    "### Set up Embeddings\n",
    "\n",
    "Mathematical representations of text (vectors). Convert all the chunks into vectors, so they can be searched efficiently. \n",
    "\n",
    "For simplicity, use `all-MiniLM-L6-v2` embedding model from `HuggingFace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d7f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af415f5",
   "metadata": {},
   "source": [
    "### ðŸ“Š Create and persist Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d9562fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created successfully at ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Define the directory where Chroma will be saved\n",
    "# This allows later reuse of the same Vector store\n",
    "persist_directory = './chroma_db'\n",
    "\n",
    "# Create the Chroma vector store from chunks\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"âœ… Vector store created successfully at {persist_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa0a47",
   "metadata": {},
   "source": [
    "### Test the vectorstore\n",
    "\n",
    "Test vectorstore with simple query to make sure, it works as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "426d37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Query: What are the best salads?\n",
      "ðŸ“š Results:\n",
      "  - restaurant_verde_garden.md: Soups & Salads...\n",
      "  - restaurant_bella_luna.md: Insalate (Salads)...\n",
      "  - restaurant_verde_garden.md: Caesar Salad - $12 Romaine lettuce, house-made Caesar dressing (cashew-based), crispy chickpeas,...\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the best salads?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"â“ Query: {query}\")\n",
    "print(\"ðŸ“š Results:\")\n",
    "for doc in docs:\n",
    "    print(f\"  - {doc.metadata['source_file']}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7123ecf",
   "metadata": {},
   "source": [
    "### âœ… Set up LangFuse for observability \n",
    "\n",
    "##### What is LangFuse?\n",
    "LangFuse is like a dashboard for AI agents. It helps you see what's happening inside your agent - what questions are asked, how it's thinking about them, and what answers are generated. This is incredibly useful for debbuging AI agents.\n",
    "\n",
    "Let's configure **LangFuse** for observabillity. We need to create a **LangFuse** Account and get an *API_KEY*.\n",
    "\n",
    "From **LangFuse Cloud** we should get `LANGFUSE_SECRET_KEY`, `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_BASE_URL`. Currently they are configured in `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6f236b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LANGFUSE_SECRET_KEY environment variable set: sk-lf...42e91\n",
      "âœ… LANGFUSE_PUBLIC_KEY environment variable set: pk-lf...22962\n",
      "âœ… LANGFUSE_BASE_URL environment variable set: https://cloud.langfuse.com\n"
     ]
    }
   ],
   "source": [
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "if not LANGFUSE_SECRET_KEY:\n",
    "    print(\"âš ï¸ LANGFUSE_SECRET_KEY environment variable not set\")\n",
    "else:\n",
    "    print(f\"âœ… LANGFUSE_SECRET_KEY environment variable set: {LANGFUSE_SECRET_KEY[:5]}...{LANGFUSE_SECRET_KEY[-5:]}\")\n",
    "\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "if not LANGFUSE_PUBLIC_KEY:\n",
    "    print(\"âš ï¸ LANGFUSE_PUBLIC_KEY environment variable not set\")\n",
    "else:\n",
    "    print(f\"âœ… LANGFUSE_PUBLIC_KEY environment variable set: {LANGFUSE_PUBLIC_KEY[:5]}...{LANGFUSE_PUBLIC_KEY[-5:]}\")\n",
    "\n",
    "LANGFUSE_BASE_URL = os.getenv(\"LANGFUSE_BASE_URL\")\n",
    "if not LANGFUSE_BASE_URL:\n",
    "    print(\"âš ï¸ LANGFUSE_BASE_URL environment variable not set\")\n",
    "else:\n",
    "    print(f\"âœ… LANGFUSE_BASE_URL environment variable set: {LANGFUSE_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb124d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LangFuse for observability\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse_client = Langfuse(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_BASE_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df844de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangFuse client created successfully\n"
     ]
    }
   ],
   "source": [
    "# Make sure we can access the LangFuse client\n",
    "from langfuse import get_client\n",
    "\n",
    "# Access the client\n",
    "langfuse_client = get_client(public_key=LANGFUSE_PUBLIC_KEY)\n",
    "print(\"âœ… LangFuse client created successfully\")\n",
    "\n",
    "# Flush all pending observations\n",
    "# This ensures all data is sent to Langfuse\n",
    "langfuse_client.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c13cd0",
   "metadata": {},
   "source": [
    "### ðŸ¤– Create a Restaurant Recommendation Agent\n",
    "\n",
    "Create `Strands` agent that uses the Chroma vector database to provide restaurants recommendations. The agent will:\n",
    "\n",
    "1. Receive question from users about restaurants\n",
    "2. Search our vector database for relevant information\n",
    "3. Generate helpful responses based on search results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f043ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for agent\n",
    "from strands.agent import Agent # The main agent class\n",
    "from strands.models.openai import OpenAIModel # OpenAI model\n",
    "from strands.tools import tool # Tool decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43203c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI model\n",
    "openai_model = OpenAIModel(\n",
    "    # Set up authentication using API key from environment variables \n",
    "    client_args={\n",
    "        'api_key': OPENAI_API_KEY\n",
    "    },\n",
    "    # Model config\n",
    "    model_id=\"gpt-4o\", # Specify which OpenAI model version to use\n",
    "    params={\n",
    "        'temperature': 0.0, # Set temperature to 0 for deterministic responses\n",
    "        'max_tokens': 2048 # Set max response length \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd352743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.hooks import HookProvider, HookRegistry\n",
    "from strands.hooks.events import AfterToolCallEvent, BeforeToolCallEvent\n",
    "\n",
    "class ToolLoggingHook(HookProvider):\n",
    "    \"\"\"\n",
    "    A simple hook to log tool names and arguments before they are executed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def register_hooks(self, registry: HookRegistry) -> None:\n",
    "        # Register the callback for the 'BeforeToolCallEvent'\n",
    "        registry.add_callback(BeforeToolCallEvent, self.log_tool_usage)\n",
    "        registry.add_callback(AfterToolCallEvent, self.log_tool_result)\n",
    "\n",
    "    def log_tool_usage(self, event: BeforeToolCallEvent) -> None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ðŸš€  **INVOKING TOOL:** {event.tool_use['name']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"ðŸ“¥  **Arguments:**\")\n",
    "        print(self._pretty_format(event.tool_use[\"input\"]))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    def log_tool_result(self, event: AfterToolCallEvent) -> None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"âœ…  **TOOL COMPLETED:** {event.tool_use['name']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"ðŸ“¤  **Result:**\")\n",
    "        # Handle cases where result might be a dict or a plain string\n",
    "        try:\n",
    "            # If the result is a JSON string/dict, format it\n",
    "            content = event.result['content']\n",
    "            if isinstance(content, (dict, list)):\n",
    "                 print(self._pretty_format(content))\n",
    "            else:\n",
    "                 print(f\"    {content}\")\n",
    "        except Exception:\n",
    "            # Fallback for complex objects\n",
    "            print(f\"    {event.result}\")\n",
    "            \n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    def _pretty_format(self, data) -> str:\n",
    "        \"\"\"Helper to format dicts/lists as indented JSON string.\"\"\"\n",
    "        try:\n",
    "            return json.dumps(data, indent=4, ensure_ascii=False)\n",
    "        except TypeError:\n",
    "            return str(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e052fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retrieval tool using Chroma DB\n",
    "# This tool will allow the agent to search for relevant information in the vector database\n",
    "@tool\n",
    "def restaurant_retrieval(query: str) -> str:\n",
    "    \"\"\"Search for restaurant information based on cuisine, dietary preferences, location or other criteria\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "\n",
    "    Returns:\n",
    "        str: Information about restaurants matching the query\n",
    "    \"\"\"\n",
    "    # Load persisted Chroma DB\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "    # Perform similarity search\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "    # Format search results as a string\n",
    "    results = \"\\n\".join([f\"{doc.metadata['source_file']}: {doc.page_content}\" for doc in docs])\n",
    "\n",
    "    return results if results else \"No relevant information found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce841dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # For generating unique IDs\n",
    "\n",
    "# Create Restaurant Recommendation Agent\n",
    "agent = Agent(\n",
    "    model=openai_model,\n",
    "    name='Restaurant Recommendation Agent',\n",
    "    tools=[restaurant_retrieval], # Give the agent access to retrieval tool\n",
    "    hooks=[ToolLoggingHook()],\n",
    "    system_prompt=\"\"\"You are a helpful restaurant recommendation assistant.\n",
    "    Use the restaurant_retrieval tool to find information about restaurants in your database.\n",
    "    Provide detailed recommendations based on user queries and search results.\n",
    "\n",
    "    If asked about restaurants that are NOT in the database, politely explain that you can only provide information about restaurants in your database.\n",
    "\n",
    "    Always be friendly, helpful and concise in your responses.\n",
    "    \"\"\",\n",
    "    callback_handler=None,\n",
    "    record_direct_tool_call=True,\n",
    "    trace_attributes={\n",
    "        'session.id': str(uuid.uuid4()), # Generate Unique session ID\n",
    "        'user.id': 'dimodrangov@gmail.com', # Example user ID\n",
    "        'langfuse.tags': [\n",
    "            'Agent-SDK Example',\n",
    "            'Restaurant-Recommendation',\n",
    "            'Strands-Project-Demo',\n",
    "            'Observability-Tutorial'\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d703a",
   "metadata": {},
   "source": [
    "### ðŸ§ª Test the agent with traces\n",
    "\n",
    "Test the agent with a simple query and see how it performs. The agent will use the retrieval tool to find relevant information and generate a response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18da55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€  **INVOKING TOOL:** restaurant_retrieval\n",
      "--------------------------------------------------\n",
      "ðŸ“¥  **Arguments:**\n",
      "{\n",
      "    \"query\": \"vegetarian options\"\n",
      "}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/x11cjlw56ln82_76czqsvqvr0000gn/T/ipykernel_97190/986610298.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "âœ…  **TOOL COMPLETED:** restaurant_retrieval\n",
      "--------------------------------------------------\n",
      "ðŸ“¤  **Result:**\n",
      "[\n",
      "    {\n",
      "        \"text\": \"restaurant_verde_garden.md: Specialties\\n\\n100% Plant-Based: Everything on our menu is vegan - no animal products whatsoever\\nrestaurant_bella_luna.md: We offer gluten-free pasta options and can accommodate vegetarian requests. Please inform your\\nrestaurant_dragon_palace.md: with vegetables and choice of protein\"\n",
      "    }\n",
      "]\n",
      "==================================================\n",
      "\n",
      "Agent response:\n",
      "Here are some restaurants with good vegetarian options:\n",
      "\n",
      "1. **Verde Garden**: This restaurant is 100% plant-based, meaning everything on their menu is vegan and free from animal products. It's a great choice if you're looking for a fully vegetarian dining experience.\n",
      "\n",
      "2. **Bella Luna**: They offer gluten-free pasta options and can accommodate vegetarian requests. Just inform your server, and they'll be happy to assist.\n",
      "\n",
      "3. **Dragon Palace**: They offer dishes with vegetables and a choice of protein, which can be customized to suit vegetarian preferences.\n",
      "\n",
      "These options should provide a variety of vegetarian-friendly dishes to enjoy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with a simple query\n",
    "query = \"I'm looking for a restaurant with goot vegetarian options. Any recommendations\"\n",
    "response = agent(query)\n",
    "print(f'Agent response:\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005f9ab",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Review the traces\n",
    "\n",
    "After running the agent review the traces in Langfuse:\n",
    "\n",
    "1. Go to the tracing menu in **LangFuse** project.\n",
    "2. Select the trace.\n",
    "3. Examine how the agent processes the request, what tools it used, and what response it generated.\n",
    "\n",
    "This gives visibility info how the agent is working and helps identifying any issues or areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536d4aa",
   "metadata": {},
   "source": [
    "### ðŸ¤” Set up **RAGAS** for evaluation \n",
    "\n",
    "Using **RAGAS** for evaluating the quality of agent's responses. \n",
    "**RAGAS** (Retrieval Augmented Generation Assesment) is a framework for evaluating **RAG** systems. \n",
    "\n",
    "##### What is RAGAS?\n",
    "RAGAS helps us measure how well our agent is performing by looking at different aspects of its responses:\n",
    "- Is the information relevant?\n",
    "- Is it relevant to the user's question?\n",
    "- Is it using the right tools?\n",
    "- Is it communicating in a friendly way?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bf53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/x11cjlw56ln82_76czqsvqvr0000gn/T/ipykernel_97190/1471895289.py:5: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluation_llm = LangchainLLMWrapper(openai_model)\n"
     ]
    }
   ],
   "source": [
    "# Import RAGAS library\n",
    "from openai import OpenAI\n",
    "from ragas.llms import llm_factory\n",
    "\n",
    "# Setup the evaluation LLM\n",
    "evaluation_llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key=OPENAI_API_KEY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61380a",
   "metadata": {},
   "source": [
    "### Define RAGAS metrics\n",
    "\n",
    "Define several metrics to evaluate different aspects of our agent's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9104f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics._aspect_critic import AspectCritic\n",
    "\n",
    "# Metric to check if the agent fulfills all user requests\n",
    "request_completeness = AspectCritic(\n",
    "    name=\"request_completeness\",\n",
    "    definition=\"Return 1 if the agent completely fulfills all the user requests with no ommisions, otherwise return 0\",\n",
    "    llm=evaluation_llm\n",
    ")\n",
    "\n",
    "# Metric to assess if the AI's communication alligns with the desired brand voice\n",
    "brand_tone = AspectCritic(\n",
    "    name=\"brand_tone\",\n",
    "    definition=\"Return 1 if the AI's communication is Friendly, approachable, helpful, clear and concise, otherwise return 0\",\n",
    "    llm=evaluation_llm\n",
    ")\n",
    "\n",
    "# Tool usage effectiveness\n",
    "tool_effectiveness = AspectCritic(\n",
    "    name=\"tool_effectiveness\",\n",
    "    definition=\"\"\"Return 1 if the agent appropriately used available tools to fulfill the user request\n",
    "    (such as using retrieve for menu questions and current_time for time questions).\n",
    "    Return 0 if the agent failed to use appropriate tools to fulfill the user request\"\"\",\n",
    "    llm=evaluation_llm\n",
    ")\n",
    "\n",
    "# Tool selection appropriateness\n",
    "tool_selection = AspectCritic(\n",
    "    name=\"tool_selection\",\n",
    "    definition=\"\"\"Return 1 if the agent appropriately selected the right tools to fulfill the user request\n",
    "    (such as using retrieve for menu questions and current_time for time questions).\n",
    "    Return 0 if the agent failed to select the right tools to fulfill the user request\"\"\",\n",
    "    llm=evaluation_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30008d9",
   "metadata": {},
   "source": [
    "Let's also define metric to evaluate RAG (Retrievement Augmented Generation) of our agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2458d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39880ec3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "restaurant-strands-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
